/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package com.npg.skalator

import org.apache.spark.sql.SparkSession
import java.sql.Timestamp
object App {

  def main(args: Array[String]): Unit = {
    if (args.length != 2){
      println("Insufficient arguments. arg1=topic, arg2=output_path")
    }

    val ss = SparkSession
      .builder
      .appName("Spark Kafka Consumer n_n)")
      .getOrCreate()

    val topicToUse = args(0)
    val outputPathToUse = args(1)
    val start = Timestamp.valueOf("2021-07-13 18:45:17.767")
    val end   = Timestamp.valueOf("2021-07-13 18:45:23.946")

    val df = ss
      .read
      .format("kafka")
      .option("kafka.bootstrap.servers", "klooster-03-w-0:9092")
      .option("subscribe", topicToUse)
      .option("enable.auto.commit","true")
      .load()
    df.printSchema()
    /*
    root
     |-- key: binary (nullable = true)
     |-- value: binary (nullable = true)
     |-- topic: string (nullable = true)
     |-- partition: integer (nullable = true)
     |-- offset: long (nullable = true)
     |-- timestamp: timestamp (nullable = true)
     |-- timestampType: integer (nullable = true)
     */

    val df_timeboxed = df.filter( e => {
      val thisTimestamp = e.getTimestamp(5)
      thisTimestamp.after(start) && thisTimestamp.before(end)
    })

    val df2 = df_timeboxed.selectExpr("CAST(key as STRING)", "CAST(value as STRING)", "timestamp","topic")

    df2.rdd.saveAsTextFile(outputPathToUse)

  }

  def greeting(): String = "Hello, world!"

}
